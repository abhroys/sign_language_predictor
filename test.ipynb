{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading label map...\n",
      "Extracting keypoints from video: D:\\data_ISL\\new_project\\data_all\\Rulers\\a2_v2.mp4\n",
      "Running inference...\n",
      "Predicted word: rulers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:11:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "from sklearn.preprocessing import LabelEncoder  # Corrected import for LabelEncoder\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = r\"D:\\data_ISL\\new_project\\models\\xgboost_xgboost.pickle.dat\"\n",
    "LABEL_MAP_PATH = r\"D:\\data_ISL\\new_project\\label_maps\\label_map.json\"\n",
    "VIDEO_PATH = r\"D:\\data_ISL\\new_project\\data_all\\Rulers\\a2_v2.mp4\"\n",
    "\n",
    "with open(LABEL_MAP_PATH, \"r\") as f:\n",
    "        label_map = json.load(f)\n",
    "\n",
    "# Updated: Flatten and Pad Function\n",
    "def flatten_and_pad(keypoints, max_seq_len=200):\n",
    "    \"\"\"\n",
    "    Flatten and pad keypoints to match the training logic.\n",
    "    Ensures consistent length for each video.\n",
    "    \"\"\"\n",
    "    keypoints = np.array(keypoints)\n",
    "    if keypoints.shape[0] < max_seq_len:\n",
    "        padded_keypoints = np.pad(\n",
    "            keypoints,\n",
    "            ((0, max_seq_len - keypoints.shape[0]), (0, 0), (0, 0)),  # Pad along the time axis\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        padded_keypoints = keypoints[:max_seq_len]  # Truncate to max_seq_len\n",
    "\n",
    "    return padded_keypoints.flatten()\n",
    "\n",
    "\n",
    "# Updated: Process Video Function\n",
    "def extract_keypoints_from_video(video_path, max_seq_len=200):\n",
    "    \"\"\"\n",
    "    Extract and process keypoints from a video using MediaPipe.\n",
    "    Ensures consistent keypoints structure.\n",
    "    \"\"\"\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5)\n",
    "    keypoints = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(frame_rgb)\n",
    "\n",
    "        # Extract landmarks or use zeros for missing landmarks\n",
    "        pose = [[0, 0]] * 33\n",
    "        left_hand = [[0, 0]] * 21\n",
    "        right_hand = [[0, 0]] * 21\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            pose = [\n",
    "                [landmark.x, landmark.y] for landmark in results.pose_landmarks.landmark\n",
    "            ]\n",
    "\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand = [\n",
    "                [landmark.x, landmark.y] for landmark in results.left_hand_landmarks.landmark\n",
    "            ]\n",
    "\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand = [\n",
    "                [landmark.x, landmark.y] for landmark in results.right_hand_landmarks.landmark\n",
    "            ]\n",
    "\n",
    "        # Combine all keypoints into a single array\n",
    "        combined = pose + left_hand + right_hand\n",
    "        keypoints.append(combined)\n",
    "\n",
    "    cap.release()\n",
    "    holistic.close()\n",
    "\n",
    "    # Ensure keypoints array is consistent\n",
    "    keypoints = np.array(keypoints)\n",
    "\n",
    "    # Check for consistency\n",
    "    if keypoints.shape[1:] != (75, 2):\n",
    "        raise ValueError(f\"Expected shape (75, 2), but got {keypoints.shape[1:]}\")\n",
    "\n",
    "    # Flatten and pad keypoints\n",
    "    return flatten_and_pad(keypoints, max_seq_len)\n",
    "\n",
    "# Inference Pipeline\n",
    "def inference_pipeline(video_path, model_path, label_map_path):\n",
    "    \"\"\"\n",
    "    Full inference pipeline for extracting keypoints and making predictions.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the model\n",
    "    print(\"Loading model...\")\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Step 2: Load the label map\n",
    "    print(\"Loading label map...\")\n",
    "    with open(label_map_path, \"r\") as f:\n",
    "        label_map = eval(f.read())  # Assuming the label map is a JSON-like dictionary\n",
    "\n",
    "    # Reverse the label map for decoding predictions\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    # Step 3: Extract keypoints from the video\n",
    "    print(f\"Extracting keypoints from video: {video_path}\")\n",
    "    keypoints = extract_keypoints_from_video(video_path)\n",
    "    keypoints = keypoints.reshape(1, -1)  # Reshape for model input\n",
    "\n",
    "    # Step 4: Model inference\n",
    "    print(\"Running inference...\")\n",
    "    prediction = model.predict(keypoints)[0]\n",
    "\n",
    "    # Step 5: Decode the prediction\n",
    "    predicted_word = reverse_label_map[prediction]\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    predicted_word = inference_pipeline(VIDEO_PATH, MODEL_PATH, LABEL_MAP_PATH)\n",
    "    print(f\"Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.18\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(mp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
